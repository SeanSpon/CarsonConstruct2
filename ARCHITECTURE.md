# Carson Construct 2 - Complete Architecture Documentation

## ğŸ¯ MAIN GOAL

**Create the most efficient, accurate, and profitable podcast clip detection system that:**
1. Identifies viral-worthy moments with 90%+ accuracy
2. Processes videos 10x faster than manual editing
3. Provides actionable insights that content creators can immediately monetize
4. Scales to handle enterprise-level batch processing

**Success Metric:** Content creators using this tool should see measurable increase in engagement (views, shares, conversions) on exported clips compared to manual selection.

---

## ğŸ“Š Executive Summary

This project consists of **two complementary Electron applications** for podcast content optimization:

### 1. **Clipper Studio** (AI-Free, Fast)
- **Purpose:** Rapid clip detection using pure algorithmic approach
- **Target Users:** Content creators who need speed and don't want AI costs
- **Key Features:** 2 pattern detectors (Payoff + Monologue), feature cache, VAD boundary snapping, speech gate, clipworthiness ensemble scoring
- **Processing Time:** ~30-60 seconds for 1-hour podcast
- **Cost:** $0 per video

### 2. **PodFlow Studio** (AI-Enhanced, Comprehensive)
- **Purpose:** Production-grade clip finder with AI semantic understanding
- **Target Users:** Professional creators/agencies willing to pay for accuracy
- **Key Features:** Payoff + Monologue + Laughter + Debate detectors, feature cache, VAD boundary snapping, speech gate, clipworthiness ensemble scoring, Whisper transcription + Translator/Thinker meaning cards + caching
- **Processing Time:** ~2-5 minutes for 1-hour podcast
- **Cost:** ~$0.50 per video (Whisper + GPT API calls)

Both applications share the same core architecture with different feature sets.

---

## ğŸ—ï¸ System Architecture Overview

### High-Level Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                            â”‚
â”‚                     (Electron + React + TS)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  File Select â”‚  â”‚  Processing  â”‚  â”‚    Review    â”‚          â”‚
â”‚  â”‚     Page     â”‚â†’ â”‚     Page     â”‚â†’ â”‚     Page     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                          â†• IPC (contextBridge)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MAIN PROCESS (Node.js)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ File Handlers   â”‚  â”‚  Detection   â”‚  â”‚   Export     â”‚       â”‚
â”‚  â”‚ - Video select  â”‚  â”‚  Handlers    â”‚  â”‚  Handlers    â”‚       â”‚
â”‚  â”‚ - File info     â”‚  â”‚ - Spawn Py   â”‚  â”‚ - FFmpeg     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â†• Child Process (spawn)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PYTHON DETECTION PIPELINE                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    detector.py (Main)                     â”‚   â”‚
â”‚  â”‚  1. Extract audio (FFmpeg)                               â”‚   â”‚
â”‚  â”‚  2. Load & normalize (librosa)                           â”‚   â”‚
â”‚  â”‚  3. Run pattern detectors                                â”‚   â”‚
â”‚  â”‚  4. Score & rank clips                                   â”‚   â”‚
â”‚  â”‚  5. [Optional] AI enhancement                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         PATTERN DETECTION + SCORING MODULES              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚   Payoff     â”‚  â”‚  Monologue   â”‚  â”‚   Laughter   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  Detection   â”‚  â”‚  Detection   â”‚  â”‚  Detection   â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚   Silence    â”‚  â”‚   Debate     â”‚  â”‚  Clip Score  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  Detection   â”‚  â”‚  Detection   â”‚  â”‚   Scoring    â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              AI ENHANCEMENT LAYER (Optional)             â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚   Whisper    â”‚â†’ â”‚ Translator   â”‚â†’ â”‚   Thinker    â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ Transcriptionâ”‚  â”‚ MeaningCard  â”‚  â”‚  Top N Set   â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXPORT PIPELINE (FFmpeg)                      â”‚
â”‚  - Individual clips (fast stream copy or accurate re-encode)    â”‚
â”‚  - Full edited video (silence removal)                          â”‚
â”‚  - Metadata JSON (titles, timestamps, scores)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Layer 1: Frontend (Renderer Process)

### Technology Stack
- **Framework:** React 19.2.3 + TypeScript 4.5.4
- **Routing:** React Router DOM 7.12.0 (HashRouter for Electron)
- **State Management:** Zustand 5.0.10 (with persist middleware for settings)
- **Styling:** Tailwind CSS 4.1.18 (Clipper) / 3.4.1 (PodFlow)
- **Icons:** Lucide React 0.562.0
- **Build Tool:** Vite 5.4.21

### State Architecture (Zustand Store)

**Clipper Studio Store:**
```typescript
{
  // File metadata
  filePath: string | null
  fileName: string | null
  fileSize: number
  fileDuration: number
  
  // Detection state
  isDetecting: boolean
  detectionProgress: { step: string, progress: number, message: string } | null
  detectionError: string | null
  
  // Results
  clips: DetectedClip[]  // All detected clips with scores
  waveform: number[]     // Visualization data (1000 points)
  
  // Export state
  isExporting: boolean
  exportProgress: { current: number, total: number } | null
}
```

**PodFlow Studio Store (Extended):**
```typescript
{
  // + All Clipper fields
  
  // Additional results
  deadSpaces: DeadSpace[]      // Silence regions for auto-edit
  transcript: Transcript | null // Whisper output with word timestamps
  
  // Settings (persisted)
  settings: {
    targetCount: number          // Number of clips to find (default: 10)
    minDuration: number          // Min clip length in seconds (default: 15)
    maxDuration: number          // Max clip length in seconds (default: 90)
    skipIntro: number            // Skip first N seconds (default: 90)
    skipOutro: number            // Skip last N seconds (default: 60)
    useAiEnhancement: boolean    // Enable Whisper + GPT (default: true)
  }
  
  exportSettings: {
    format: 'mp4' | 'mov'
    mode: 'fast' | 'accurate'
    exportClips: boolean
    exportFullVideo: boolean
  }
  
  // Recent projects (persisted)
  recentProjects: Array<{
    filePath: string
    fileName: string
    duration: number
    lastOpened: number
  }>
}
```

### UI Pages Flow

#### Clipper Studio:
1. **SelectFile** â†’ Upload/select video â†’ Extract metadata
2. **Processing** â†’ Real-time progress updates â†’ Pattern detection
3. **Review** â†’ Clip cards with scores â†’ Trim/filter/export

#### PodFlow Studio:
1. **Home** â†’ Recent projects + New project + Settings
2. **ClipFinder** â†’ Detection + Results grid + Score breakdown
3. **AutoEdit** â†’ Dead space timeline + Toggle remove/keep
4. **Export** â†’ Format selection + Export progress

### IPC Communication Pattern

**Renderer â†’ Main (invoke):**
```typescript
// File selection
window.api.selectVideoFile() â†’ { path, name, size, duration }

// Start detection
window.api.startDetection(filePath, settings) â†’ { success, error? }

// Export clips
window.api.exportClips(clips, settings) â†’ { success, outputDir, error? }
```

**Main â†’ Renderer (send):**
```typescript
// Progress updates (streaming)
detection-progress â†’ { progress: 0-100, message: string }

// Results
detection-complete â†’ { clips: Clip[], deadSpaces?: DeadSpace[], transcript?: Transcript }

// Errors
detection-error â†’ { error: string }

// Export progress
export-progress â†’ { current: number, total: number, clipName: string }
export-complete â†’ { success: boolean, outputDir: string }
```

### ğŸ” **RESEARCH QUESTIONS - Frontend Layer:**

1. **State Management Performance:**
   - Q: Would using Jotai or Valtio reduce re-renders when updating large clip arrays (100+ clips)?
   - Research: Benchmark Zustand vs Jotai vs Valtio with 200+ clips being scored in real-time
   - Expected Impact: 10-20% UI responsiveness improvement

2. **Waveform Visualization:**
   - Q: Should we use Canvas API, WebGL (via Three.js), or SVG for waveform rendering?
   - Current: Simple array of 1000 points
   - Research: Performance comparison for interactive waveforms with 10k+ data points
   - Expected Impact: Smoother scrubbing, better UX for long videos (3+ hours)

3. **Virtual Scrolling:**
   - Q: Do we need React Virtual or Tanstack Virtual for clip lists?
   - Threshold: When does it matter? (20 clips? 50? 100?)
   - Research: Measure FPS and memory usage with 100+ ClipCard components
   - Expected Impact: Critical for batch processing UI

4. **Progressive Web App (PWA) Alternative:**
   - Q: Could this be a web app with server-side Python processing instead of Electron?
   - Trade-offs: File access, FFmpeg bundling, offline usage, distribution
   - Research: Cost-benefit analysis of Electron vs PWA + Python backend
   - Expected Impact: Potential 50% bundle size reduction, easier updates

5. **Real-Time Audio Preview:**
   - Q: Should we implement in-app audio/video playback with Web Audio API?
   - Current: Users must use external player
   - Research: Electron video codecs support, memory overhead, seek performance
   - Expected Impact: Major UX improvement, 30% faster workflow

---

## âš™ï¸ Layer 2: Main Process (Electron + Node.js)

### Technology Stack
- **Runtime:** Electron 40.0.0
- **Build:** Electron Forge 7.11.1
- **IPC:** contextBridge + ipcMain/ipcRenderer
- **Child Process:** Node.js spawn for Python scripts

### IPC Handlers Architecture

**1. File Handlers (`src/main/ipc/fileHandlers.ts`):**
```typescript
// File selection with native dialog
ipcMain.handle('select-video-file') â†’ 
  - electron.dialog.showOpenDialog()
  - Validate file format (.mp4, .mov, .avi, .mkv)
  - Extract metadata (FFprobe or file stats)
  - Return { path, name, size, duration }

// Directory selection for exports
ipcMain.handle('select-output-directory') â†’
  - electron.dialog.showOpenDialog({ properties: ['openDirectory'] })
```

**2. Detection Handlers (`src/main/ipc/detectionHandlers.ts`):**
```typescript
// Spawn Python detector process
ipcMain.handle('start-detection', (event, { projectId, filePath, settings }) â†’
  1. Find Python script path (development vs production)
  2. Serialize settings to JSON
  3. spawn('python', [detectorScript, filePath, settingsJson])
  4. Parse stdout for PROGRESS/RESULT/ERROR messages
  5. Throttle progress IPC (max 10/s, step changes pass immediately)
  6. Forward to renderer via webContents.send()

// Kill running detection
ipcMain.handle('cancel-detection', (projectId) â†’
  - process.kill('SIGTERM')
  - Clean up activeProcesses Map
```

**3. Export Handlers (`src/main/ipc/exportHandlers.ts`):**
```typescript
// Export individual clips or full video
ipcMain.handle('export-clips', (clips, outputDir, settings) â†’
  1. For each clip:
     - Calculate FFmpeg timecode with trim offsets
     - Fast mode: -c copy (stream copy, no re-encode)
     - Accurate mode: -c:v libx264 -c:a aac (re-encode)
  2. For full video with dead space removal:
     - Build complex filter_complex with concat
  3. Progress tracking via FFmpeg stderr parsing
  4. Export metadata JSON (titles, scores, timestamps)
```

### Process Communication Flow

**Clipper Studio:**
```
Renderer â†’ Main â†’ Python â†’ Main â†’ Renderer

1. User clicks "Detect Clips"
2. Renderer: startDetection(filePath)
3. Main: spawn('python', ['detector.py', filePath])
4. Python: Writes JSON to stdout
   - {"type": "progress", "progress": 30, "message": "Detecting payoffs..."}
   - {"type": "complete", "clips": [...], "waveform": [...]}
5. Main: Parses JSON, sends IPC to renderer
6. Renderer: Updates Zustand store, re-renders UI
```

**PodFlow Studio (with AI):**
```
Same as above, but Python script:
1. Runs algorithm detectors (fast, ~60s)
2. Calls OpenAI Whisper API (slow, ~90s for 1hr audio)
3. Calls GPT-4o-mini for each clip (fast, ~10s total)
4. Returns enhanced clips with AI metadata
```

### ğŸ” **RESEARCH QUESTIONS - Main Process Layer:**

1. **Python Distribution:**
   - Q: How should we bundle Python for production? (Embedded Python? PyInstaller? User-installed?)
   - Current: Assumes user has Python in PATH
   - Research: Compare approaches used by OBS Studio, Davinci Resolve
   - Expected Impact: Critical for non-technical users

2. **Multiprocessing:**
   - Q: Can we parallelize pattern detection across CPU cores?
   - Current: Single Python process, sequential detection
   - Research: Python multiprocessing vs threading for audio analysis
   - Expected Impact: Potential 2-4x speedup on multi-core systems

3. **Streaming vs Batch IPC:**
   - Q: Should we buffer progress updates or send immediately?
   - Current: Progress updates are throttled to ~10/s with step-change bypass
   - Research: Debounce/throttle strategies, measure overhead
   - Expected Impact: Reduced CPU usage on main thread

4. **Native Modules:**
   - Q: Should FFmpeg be a Node.js native binding instead of CLI spawning?
   - Options: fluent-ffmpeg, @ffmpeg-installer/ffmpeg, native binding
   - Research: Performance difference, maintenance complexity
   - Expected Impact: Potential 20-30% faster exports

5. **Process Pooling:**
   - Q: For batch processing, should we maintain a pool of Python workers?
   - Current: Spawn new process for each video
   - Research: Worker pool patterns, process lifecycle management
   - Expected Impact: Critical for enterprise batch mode

---

## ğŸ Layer 3: Python Detection Pipeline

### Technology Stack
- **Audio Analysis:** librosa 0.10.1 (STFT, RMS, spectral features)
- **Numerical Computing:** numpy 1.26.3, scipy 1.12.0
- **AI (PodFlow only):** openai 1.12.0 (Whisper + GPT-4o-mini)
- **Audio I/O:** soundfile 0.12.1
- **Video Processing:** FFmpeg (subprocess calls)
- **VAD:** webrtcvad (speech segmentation, boundary snapping)

### Algorithmic Enhancements (2026-01-18)
- **Feature cache:** `features.py` computes RMS, centroid, flatness, ZCR, onset once and reuses everywhere.
- **VAD utilities:** `vad_utils.py` builds speech segments (WebRTC VAD with fallback) and always attempts boundary snapping (no UI toggle; clips stay unchanged if no segments).
- **Local baselines:** `utils/baseline.py` applies rolling medians for RMS/centroid/ZCR/onset thresholds.
- **Speech gate:** hard gates block low speech ratio or high flatness (noise/music); reject reasons surface only in debug output.
- **Clipworthiness scoring:** `utils/clipworthiness.py` applies hard gates + soft-score ensemble; clip breakdown is always attached, with extra debug metrics when enabled.
- **Debate detector (PodFlow):** `patterns/debate.py` detects rapid turn-taking with short gaps.
- **Debug toggle:** gate reasons and snap diagnostics are returned only when debug is enabled.

### Main Pipeline (`detector.py`)

**Clipper Studio Pipeline (Simple):**
```python
def main(video_path: str):
    1. extract_audio(video_path, tmpdir)
       â†’ FFmpeg: -vn -acodec pcm_s16le -ar 22050 -ac 1
       â†’ Output: mono WAV at 22.05kHz

    2. y, sr = librosa.load(audio_path, sr=22050)
       â†’ Load audio time series
       â†’ duration = librosa.get_duration(y=y, sr=sr)

    3. features = extract_features(y, sr)
       â†’ Cache RMS, centroid, flatness, ZCR, onset, VAD mask

    4. payoff = detect_payoff_moments(features, bounds)
       monologue = detect_energy_monologues(features, bounds)

    5. Snap to VAD boundaries + apply speech gate
       â†’ snap_clip_to_segments(...) + apply_clipworthiness(...)

    6. scored_clips = select_final_clips(scored_clips, max_clips=20, min_gap=30)

    7. waveform = generate_waveform(y, num_points=1000)

    8. send_complete(clips=final_clips, waveform=waveform)
```

**PodFlow Studio Pipeline (Enhanced):**
```python
def main(video_path: str, settings: dict):
    1. extract_audio_ffmpeg(video_path, audio_path)

    2. y, sr = librosa.load(audio_path, sr=22050)
       y = normalize_audio(y, sr)  # Remove DC offset, normalize peak

    3. # Define analysis boundaries
       start_time = settings['skip_intro']  # Default: 90s
       end_time = duration - settings['skip_outro']  # Default: 60s

    4. features = extract_features(y, sr)
       â†’ Cache RMS/centroid/flatness/ZCR/onset + VAD segments

    5. # Run pattern detectors
       payoff_clips = detect_payoff_moments(features, bounds)
       monologue_clips = detect_energy_monologues(features, bounds)
       laughter_clips = detect_laughter_moments(features, bounds)
       debate_clips = detect_debate_moments(features, bounds)
       dead_spaces = detect_dead_spaces(features, bounds, min_silence=3.0)

    6. Snap to VAD boundaries + apply speech gate
       â†’ snap_clip_to_segments(...) + apply_clipworthiness(...)

    7. Merge overlapping clips + select top candidates
       â†’ merge_overlapping_clips(...) + select_final_clips(...)

    8. # AI Enhancement (optional)
       if settings['use_ai_enhancement']:
           transcript = transcribe_with_whisper(audio_path, openai_key)  # if key provided
           final_clips = run_ai_enhancement(final_clips, transcript, settings)

    9. # Final selection: top N clips
       # If AI disabled, sort by finalScore.
       # If AI enabled, thinker ranking already returns N clips.

    10. send_result(clips=final_clips, dead_spaces=dead_spaces, transcript=transcript)
```

### Pattern Detection Algorithms (Deep Dive)

#### 1. **Payoff Moment Detection** (`patterns/payoff.py`)

**Concept:** Silence/low-energy â†’ sudden energy spike (punchlines, reveals)

**Algorithm:**
```python
1. Calculate RMS energy in 50ms windows
   rms = librosa.feature.rms(y=y, hop_length=int(sr * 0.05))
   
2. Define LOCAL thresholds (PodFlow uses 10s baseline)
   silence_threshold = percentile(rms, 25)  # Bottom 25%
   spike_threshold = mean + 1.5*std  # 1.5 standard deviations above mean
   
3. Find silence regions (1.5-5 seconds duration)
   for each frame in rms:
       if energy < silence_threshold:
           track silence duration
       if duration in [1.5, 5.0]:
           save as candidate silence
   
4. Check what comes AFTER each silence (0.5-3 seconds)
   post_silence_energy = rms[silence_end : silence_end + 3s]
   max_energy = max(post_silence_energy)
   
   if max_energy > spike_threshold * 1.5:
       # Verify spike sustains for 0.5+ seconds
       if sustained_duration >= 0.5:
           # Score based on silence duration + spike intensity
           silence_score = (silence_duration / 5.0) * 35
           spike_score = (spike_intensity - 1.0) * 22
           sustain_score = sustained_duration * 10
           algorithm_score = silence_score + spike_score + sustain_score
   
5. Clip boundaries
   clip_start = silence_start - 5s  # Include context
   clip_end = silence_end + 5s + sustained_duration  # Include payoff + reaction
   
   # Enforce min/max duration constraints
   if clip_duration < min_duration:
       extend clip symmetrically
   if clip_duration > max_duration:
       trim to max, keeping payoff centered
```

**Key Insight:** Both apps now use rolling median baselines (10-20s windows) so thresholds adapt to local audio conditions and reduce false positives.

**Scoring Breakdown:**
- Silence score (0-35): Longer pauses = more build-up
- Spike score (0-45): Higher energy spike = bigger payoff
- Sustain score (0-20): Longer spike = not just noise

#### 2. **Energy Monologue Detection** (`patterns/monologue.py`)

**Concept:** Sustained high energy + fast speech pace (rants, hot takes, passionate segments)

**Algorithm:**
```python
1. Calculate energy envelope
   rms = librosa.feature.rms(y=y, hop_length=hop_length)
   smoothed_rms = moving_average(rms, window=20)  # 1-second smoothing
   
2. Calculate speech rate proxy (zero-crossing rate)
   zcr = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)
   # High ZCR = fast speech / complex audio
   
3. Define energy threshold
   energy_threshold = percentile(smoothed_rms, 70)  # Top 30%
   
4. Define pace threshold
   pace_threshold = percentile(zcr, 60)  # Above-average pace
   
5. Find sustained high-energy regions
   in_monologue = False
   for i, (energy, pace) in enumerate(zip(smoothed_rms, zcr)):
       if energy > energy_threshold and pace > pace_threshold:
           if not in_monologue:
               monologue_start = times[i]
           in_monologue = True
       else:
           if in_monologue and duration >= min_duration:
               # Score based on energy level + pace + duration
               energy_score = (avg_energy / energy_threshold) * 40
               pace_score = (avg_pace / pace_threshold) * 30
               duration_score = min(30, duration * 0.5)
               algorithm_score = energy_score + pace_score + duration_score
```

**Key Features:**
- Uses VAD speech density + onset deviation instead of ZCR-only pacing
- Requires BOTH high energy deviation AND dense speech (blocks music/noise)
- Rewards longer sustained segments (passion = engagement)

#### 3. **Laughter Detection** (PodFlow only, `patterns/laughter.py`)

**Concept:** Burst energy clusters with high-frequency content (comedic moments)

**Algorithm:**
```python
1. Calculate spectral centroid + RMS + ZCR + onset strength
   # Use rolling baselines for local deviations

2. Find "bright burst" patterns
   centroid_dev, energy_dev, zcr_dev, onset_dev

3. Burst clustering
   - Require multiple peaks within a 1-3s window
   - Filters out isolated spikes and music hits
```

**Key Insight:** Laughter has unique spectral signature (high centroid) + temporal pattern (bursts). This distinguishes it from music or shouts.

#### 4. **Dead Space Detection** (PodFlow only, `patterns/silence.py`)

**Concept:** Silence > 3 seconds (awkward pauses, dead air)

**Algorithm:**
```python
1. Calculate RMS energy (smoothed)

2. Define silence threshold relative to rolling baseline
   silence_deviation = (rms - baseline) / baseline

3. Find silence regions
   in_silence = False
   for i, energy in enumerate(rms):
       if energy < silence_threshold:
           track silence duration
       else:
           if silence_duration >= min_silence (3.0s):
               save as dead_space with {start, end, duration}
```

**Use Case:** Auto-edit feature removes these silences to tighten pacing.

#### 5. **Hook Strength Scoring** (Both apps, `utils/clipworthiness.py`)

**Concept:** First 3 seconds of a clip should grab attention; score uses energy lift + onset novelty.

**Algorithm:**
```python
1. hook_window = [clip_start, clip_start + 3s]
2. rms_ratio = mean(rms) / mean(rms_baseline)
3. onset_dev = deviation_from_baseline(onset, onset_baseline)
4. novelty = clamp(mean(onset_dev), 0.0, 2.0)
5. hook_score = clamp(50 + (rms_ratio - 1.0) * 35 + novelty * 15, 0, 100)
6. hook_multiplier = clamp(0.85 + (hook_score - 50) / 200, 0.85, 1.2)
```

**Impact:** hook_score feeds clipworthiness, and hook_multiplier modestly boosts final score.

#### 6. **Clipworthiness Scoring** (Both apps, `utils/clipworthiness.py`)

**Concept:** Deterministic ensemble score with hard gates + soft scores.

**Hard Gates:**
- speech_ratio â‰¥ 0.70 (VAD mask)
- spectral_flatness median â‰¤ 0.45 (noise/music filter)
- speech_seconds â‰¥ 6s within clip

**Soft Scores (0-100 each):**
- payoff_score / monologue_score / laughter_score / debate_score
- hook_score (energy deviation first 3s + novelty)
- coherence_score (VAD phrase boundary alignment)

**Final Score:**
- Weighted sum (Clipper vs PodFlow weights)
- Breakdown always attached to scored clips (`clip['clipworthiness']`)
- Debug adds gate metrics/hook ratio and gated clip details in debug payload

### AI Enhancement Layer (PodFlow only)

#### 1. **Whisper Transcription** (`ai/transcription.py`)

```python
def transcribe_with_whisper(audio_path, api_key):
    client = OpenAI(api_key=api_key)
    
    with open(audio_path, 'rb') as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["word", "segment"]
        )
    
    # Returns: {segments: [{start, end, text}], words: [{start, end, word}]}
    # Cost: ~$0.006 per minute (~$0.36 for 1-hour podcast)
```

**Use Case:** Enables semantic clip analysis (keywords, topics, speaker diarization potential).

#### 2. **Translator + Thinker Orchestration** (`ai/translator.py`, `ai/thinker.py`, `ai/orchestrator.py`)

```python
def run_ai_enhancement(candidates, transcript, settings):
    # Build ClipCard for each candidate
    # Translator: ClipCard -> MeaningCard (title/hook/category/flags)
    # Thinker: pick best N with dedupe + constraints
    # Cache MeaningCard per clip to skip repeat calls
    return selected_clips
```

- **Translator:** Converts each ClipCard into MeaningCard JSON (category, complete thought, title/hook, flags, multiplier)
- **Thinker:** Selects the best N clips with dedupe, constraints, and optional AI reasoning
- **Orchestrator:** Handles caching, fallbacks, and final score updates
- **Fallbacks:** No API key or AI errors â†’ deterministic heuristics + algorithmic ranking

### Evaluation Harness (Precision@K)

- **Runner:** `tools/eval/run_eval.py`
- **Dataset format:**
```json
{
  "episodes": [
    {
      "id": "episode-1",
      "ground_truth": [{ "start": 120.0, "end": 150.0, "label": "payoff" }],
      "predictions": [{ "start": 118.0, "end": 148.0, "pattern": "payoff", "finalScore": 92.0 }]
    }
  ]
}
```
- **Command:** `python tools/eval/run_eval.py --dataset data/sample.json --k 10`
- **Outputs:** console summary + `tools/eval/report.json`
- **Dependencies:** Python stdlib only (no extra setup)

### Benchmark Note (Local)
- Feature cache removes repeated librosa feature extraction across detectors.
- No before/after timing captured in-repo yet; run local profiling on a 1-hour episode to populate.

### Tests (Python)
- `cd podflow-studio/src/python && python -m unittest discover -s tests`

### ğŸ” **RESEARCH QUESTIONS - Python Layer:**

1. **Audio Preprocessing:**
   - Q: Should we apply noise reduction (noisereduce library) before analysis?
   - Trade-off: Improved accuracy vs processing time vs false negatives
   - Research: Test on podcasts with varying audio quality
   - Expected Impact: Potential 10-15% accuracy improvement for low-quality audio

2. **Feature Engineering:**
   - Q: Are there better features than RMS + ZCR for speech/energy detection?
   - Options: MFCCs, chromagrams, spectral rolloff, onset strength
   - Research: Test with scikit-learn feature importance on labeled dataset
   - Expected Impact: Major accuracy gains if better features found

3. **Machine Learning Alternative:**
   - Q: Should we train a CNN/RNN to detect patterns instead of hand-crafted algorithms?
   - Approach: Create labeled dataset of 1000+ clips (viral vs non-viral)
   - Research: Compare with AudioSet, VGGish, PANNs pre-trained models
   - Expected Impact: Potential 20-40% accuracy improvement, but requires labeled data

4. **Real-Time vs Batch Processing:**
   - Q: Can we do sliding-window real-time detection while video uploads?
   - Current: Wait for full audio, then process
   - Research: Streaming audio analysis, incremental results
   - Expected Impact: Perceived 50% faster (show results during processing)

5. **GPU Acceleration:**
   - Q: Would CuPy (GPU-accelerated NumPy) significantly speed up processing?
   - Current: CPU-only librosa operations
   - Research: Profile bottlenecks, estimate GPU speedup
   - Expected Impact: Potential 3-5x speedup for spectral analysis

6. **Alternative Transcription:**
   - Q: Should we use local Whisper (faster-whisper, whisper.cpp) instead of API?
   - Trade-off: One-time processing cost vs per-video API cost vs accuracy
   - Research: Compare accuracy (WER), speed, memory usage
   - Expected Impact: $0.36 â†’ $0 per video, but 2-3x slower

7. **Speaker Diarization:**
   - Q: Can we use pyannote.audio to identify which speaker has best clips?
   - Use Case: Multi-host podcasts, guest identification
   - Research: Accuracy on podcast audio, integration complexity
   - Expected Impact: New feature: "Find best Joe Rogan moments"

8. **Clip Boundary Optimization:**
   - Q: Should we use VAD (Voice Activity Detection) to snap boundaries to speech edges?
   - Current: VAD snapping (Â±2s window + tail padding), thresholds still tunable
   - Research: webrtcvad, silero-vad for precise boundaries
   - Expected Impact: Cleaner clip edges, better UX

9. **Sentiment Analysis:**
   - Q: Can we use sentiment models to prefer positive/exciting moments?
   - Options: HuggingFace transformers (DistilBERT), OpenAI embeddings
   - Research: Correlation between sentiment and viral performance
   - Expected Impact: Potential 10-20% better viral accuracy

10. **Multi-Modal Analysis:**
    - Q: Should we analyze video frames (faces, motion) in addition to audio?
    - Use Case: Detect visual reactions, gestures, screen shares
    - Research: OpenCV face detection, motion vectors from FFmpeg
    - Expected Impact: Major differentiation from audio-only tools

---

## ğŸ“¦ Layer 4: Export Pipeline (FFmpeg)

### Current Implementation

**Individual Clip Export:**
```bash
# Fast mode (stream copy, no re-encoding)
ffmpeg -ss <start> -i input.mp4 -t <duration> -c copy output_clip1.mp4

# Accurate mode (re-encode for frame-perfect cuts)
ffmpeg -i input.mp4 -ss <start> -t <duration> \
  -c:v libx264 -preset medium -crf 23 \
  -c:a aac -b:a 192k \
  output_clip1.mp4
```

**Full Video with Dead Space Removal:**
```bash
# Build filter_complex for concatenation
ffmpeg -i input.mp4 \
  -filter_complex "
    [0:v]trim=start=0:end=30[v0];
    [0:v]trim=start=40:end=80[v1];
    [0:v]trim=start=90:end=120[v2];
    [v0][v1][v2]concat=n=3:v=1:a=0[vout];
    
    [0:a]atrim=start=0:end=30[a0];
    [0:a]atrim=start=40:end=80[a1];
    [0:a]atrim=start=90:end=120[a2];
    [a0][a1][a2]concat=n=3:v=0:a=1[aout]
  " \
  -map "[vout]" -map "[aout]" \
  output_edited.mp4
```

**Metadata Export:**
```json
{
  "sourceFile": "podcast_ep42.mp4",
  "exportDate": "2026-01-18T10:30:00Z",
  "clips": [
    {
      "id": "payoff_1",
      "fileName": "podcast_ep42_clip1.mp4",
      "title": "You Won't Believe What He Said Next",
      "startTime": 245.6,
      "endTime": 263.2,
      "duration": 17.6,
      "pattern": "payoff",
      "algorithmScore": 87.3,
      "aiScore": 9.2,
      "finalScore": 91.1,
      "hookStrength": 82,
      "hookText": "So I told him to his face..."
    }
  ]
}
```

### ğŸ” **RESEARCH QUESTIONS - Export Layer:**

1. **Hardware Encoding:**
   - Q: Should we use GPU encoders (h264_nvenc, h264_qsv, h264_videotoolbox)?
   - Current: CPU-only libx264
   - Research: Quality comparison (CRF equivalent), speed improvement
   - Expected Impact: 5-10x faster exports on systems with GPU

2. **Codec Optimization:**
   - Q: Should we support H.265/HEVC for smaller file sizes?
   - Trade-off: 50% smaller files vs compatibility vs encoding time
   - Research: Social media platform support (TikTok, Instagram, YouTube)
   - Expected Impact: Bandwidth savings for batch exports

3. **Two-Pass Encoding:**
   - Q: Is two-pass encoding worth the 2x time for better quality?
   - Current: Single-pass CRF
   - Research: Perceived quality difference in A/B tests
   - Expected Impact: Better quality at same file size, but 2x slower

4. **Progressive Upload:**
   - Q: Can we upload clips to social media while still exporting others?
   - Integration: TikTok API, Instagram Graph API, YouTube Data API
   - Research: Official APIs vs third-party tools (Buffer, Hootsuite)
   - Expected Impact: Workflow optimization, but complex auth

5. **Clip Templating:**
   - Q: Should we add intros/outros, watermarks, captions automatically?
   - Use Case: Branding, accessibility, repurposing content
   - Research: FFmpeg overlay filters, caption generation (Whisper JSON)
   - Expected Impact: Major value-add for professional creators

6. **Batch Processing UI:**
   - Q: Should we support queue-based exports for multiple videos?
   - Current: One video at a time
   - Research: Job queue architecture, progress tracking
   - Expected Impact: Critical for agencies processing 10+ videos/day

---

## ğŸ”„ Complete Data Flow Example

**Scenario:** User uploads 1-hour podcast to PodFlow Studio, exports top 5 clips

### Step-by-Step Flow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 0:00 - USER UPLOADS VIDEO                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  1. User drags podcast_ep42.mp4 to Home page
  2. Renderer: window.api.selectVideoFile()
  3. Main: electron.dialog.showOpenDialog()
  4. Main: FFprobe extracts metadata â†’ duration: 3600s
  5. Renderer: Updates store with fileInfo
  6. User clicks "Analyze" â†’ navigates to ClipFinder page

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 0:05 - DETECTION STARTS                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  7. Renderer: window.api.startDetection(filePath, settings)
  8. Main: spawn('python', ['detector.py', filePath, settingsJson])
  9. Python: FFmpeg extracts audio â†’ /tmp/tmpXYZ/audio.wav
  10. Python: librosa.load() â†’ 22.05kHz mono array
  11. Python: PROGRESS:20:Audio loaded
  12. Main: Parses stdout â†’ send('detection-progress', {progress:20})
  13. Renderer: Updates progress bar to 20%

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 0:30 - PAYOFF DETECTION                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  14. Python: detect_payoff_moments(y, sr, 3600)
      - Calculates RMS energy (72,000 frames @ 50ms)
      - Finds 18 silence regions (1.5-5s duration)
      - Detects 12 valid payoff moments (silence â†’ spike)
  15. Python: PROGRESS:40:Found 12 payoff moments
  16. Renderer: Updates progress bar to 40%

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 0:45 - MONOLOGUE DETECTION                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  17. Python: detect_energy_monologues(y, sr, 3600)
      - Calculates energy + ZCR
      - Finds 8 sustained high-energy segments
  18. Python: PROGRESS:55:Found 8 monologue moments
  19. Renderer: Updates progress bar to 55%

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 1:00 - LAUGHTER DETECTION                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  20. Python: detect_laughter_moments(y, sr, 3600)
      - Calculates spectral centroid
      - Finds 5 laughter burst patterns
  21. Python: PROGRESS:65:Found 5 laughter moments
  22. Renderer: Updates progress bar to 65%

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 1:10 - SCORING & RANKING                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  23. Python: all_clips = merge_overlapping_clips([12+8+5 clips])
      â†’ 22 unique clips after merge
  24. Python: calculate_final_scores(all_clips, y, sr)
      - Adds hook strength, viralScore, engagement factors
  25. Python: select_final_clips(all_clips, max_clips=20, min_gap=30)
      â†’ 18 clips after enforcing 30s minimum gap
  26. Python: PROGRESS:75:Scoring complete

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 1:15 - AI ENHANCEMENT (Optional)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  27. Python: transcribe_with_whisper(audio_path, api_key)
      - Uploads 60-minute audio to OpenAI
      - Receives transcript with word timestamps
      - Cost: $0.36
  28. Python: PROGRESS:85:Transcription complete
  29. Python: enhance_clips_with_ai(clips, transcript, api_key)
      - For each of 18 clips:
        - Extract transcript segment
        - Call GPT-4o-mini for title + validation
      - Cost: $0.02 Ã— 18 = $0.36
  30. Python: PROGRESS:90:AI enhancement complete

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 2:30 - DETECTION COMPLETE                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  31. Python: Select top 10 clips by finalScore
  32. Python: RESULT:{clips:[...], deadSpaces:[...], transcript:{...}}
  33. Main: Parses JSON â†’ send('detection-complete', data)
  34. Renderer: Updates store.clips, renders ClipCard grid
  35. User sees 10 clips sorted by score, each with:
      - AI-generated title
      - Duration, pattern, scores
      - Hook text preview
      - Trim controls

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 2:35 - USER REVIEWS & EXPORTS                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  36. User reviews clips, unchecks 2 low-quality ones
  37. User adjusts trim on Clip #3 (remove 2s from start)
  38. User navigates to Export page
  39. User selects: Format=MP4, Mode=Fast, Export Clips=true
  40. User clicks "Export 8 Selected Clips"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 2:40 - EXPORT STARTS                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  41. Renderer: window.api.exportClips(selectedClips, settings)
  42. Main: Creates output directory with timestamp
  43. Main: For each clip (8 total):
      - Spawns FFmpeg with -ss/-t/-c copy
      - Monitors progress via stderr parsing
      - send('export-progress', {current:1, total:8, clipName:'...'})
  44. Renderer: Updates export progress bar

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ T = 2:50 - EXPORT COMPLETE                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  45. Main: Exports metadata.json with all clip info
  46. Main: send('export-complete', {success:true, outputDir:'...'})
  47. Renderer: Shows success notification with "Open Folder" button
  48. User opens folder, sees:
      - podcast_ep42_clip1.mp4 (title: "You Won't Believe...")
      - podcast_ep42_clip2.mp4 (title: "The Shocking Truth About...")
      - ... (6 more clips)
      - metadata.json
  49. User uploads clips to TikTok/Instagram with AI-generated titles
  50. ğŸ‰ Profit!
```

**Total Time:** ~2 minutes 50 seconds (with AI), ~1 minute 10 seconds (without AI)  
**Total Cost:** $0.72 (Whisper $0.36 + GPT $0.36)

---

## ğŸ¯ Critical Success Metrics

### Performance Benchmarks

| Metric | Clipper Studio (Target) | PodFlow Studio (Target) | Current Status |
|--------|------------------------|-------------------------|----------------|
| **Detection Speed** | 30-60s for 1hr podcast | 60-180s for 1hr podcast | âœ… Meeting target |
| **Accuracy** | 70% viral prediction | 85% viral prediction | âš ï¸ Need validation dataset |
| **Memory Usage** | < 500MB peak | < 1GB peak | âœ… Within limits |
| **Export Speed** | 5-10s per clip (fast) | 5-10s per clip (fast) | âœ… Meeting target |
| **False Positive Rate** | < 30% | < 15% | â“ Unknown (need user feedback) |
| **Cost per Video** | $0 | $0.50-$1.00 | âœ… Meeting target |

### Business Metrics

| Metric | Target | How to Measure |
|--------|--------|----------------|
| **User Retention** | 60% weekly active | Analytics, usage tracking |
| **Viral Hit Rate** | 1 in 5 clips gets 10k+ views | User surveys, case studies |
| **Time Saved** | 80% vs manual editing | User surveys (before/after) |
| **Monetization Potential** | SaaS ($29/mo) or Pro version ($99 one-time) | Market research, competitor analysis |

---

## ğŸ›¤ï¸ PATHS TO ACHIEVE MAIN GOAL

### Path 1: **Machine Learning Approach** (High Risk, High Reward)

**Strategy:** Replace hand-crafted algorithms with trained models

**Steps:**
1. Build labeled dataset (1000+ clips: viral vs non-viral)
2. Train CNN/RNN on spectral features (AudioSet, PANNs)
3. Fine-tune on podcast-specific data
4. A/B test against current algorithms

**Pros:**
- Potential 20-40% accuracy improvement
- Learns patterns humans can't articulate
- Improves over time with more data

**Cons:**
- Requires significant labeled data (expensive, time-consuming)
- Model deployment complexity (TensorFlow.js, ONNX, or Python-only)
- Black box (harder to debug/explain)
- Longer processing time (model inference)

**Research Questions:**
- Where to get labeled data? (Manual labeling? Social media metrics?)
- Which architecture? (AudioCLIP, wav2vec2, custom CNN?)
- How to define "viral"? (Views? Engagement rate? Share count?)

### Path 2: **Semantic Understanding** (Moderate Risk, High Value)

**Strategy:** Deep integration with transcript analysis and NLP

**Steps:**
1. Transcribe all audio (local Whisper for cost efficiency)
2. Extract keywords, topics, entities (spaCy, BERT)
3. Sentiment analysis for emotional peaks
4. Speaker diarization to identify key personalities
5. Cross-reference with social media trends (trending topics API)

**Pros:**
- Explains WHY clip is good (not just THAT it's good)
- Enables filtering (e.g., "Find clips about crypto")
- Better titles/metadata for SEO
- Differentiates from audio-only tools

**Cons:**
- Transcription adds 60-90s processing time
- NLP models require GPU for real-time
- False positives if transcript quality is poor

**Research Questions:**
- Local Whisper vs API: which is better for this use case?
- Can we use LLM embeddings (OpenAI, Cohere) for clip similarity?
- How to integrate real-time trend data? (Google Trends, Twitter API)

### Path 3: **Multi-Modal Analysis** (High Risk, Major Differentiation)

**Strategy:** Analyze video frames in addition to audio

**Steps:**
1. Extract keyframes at 1fps (FFmpeg)
2. Face detection + emotion recognition (DeepFace, FER)
3. Object detection for context (YOLO, DETR)
4. Motion analysis (optical flow, scene changes)
5. Screen share detection (for podcast with presentations)
6. Combine audio + visual scores

**Pros:**
- Unique differentiator (no competitors do this well)
- Captures visual jokes, reactions, gestures
- Better for video podcasts (Joe Rogan, Lex Fridman)

**Cons:**
- Massive complexity increase
- GPU required for real-time
- Longer processing time (3-5 minutes for 1hr video)
- Higher memory usage (2-4GB)

**Research Questions:**
- Which visual features correlate with virality? (Faces? Motion? Color?)
- Can we use CLIP (OpenAI) for multi-modal embeddings?
- Do visual features improve accuracy enough to justify complexity?

### Path 4: **Hybrid Optimization** (Low Risk, Incremental Improvement) â­ RECOMMENDED

**Strategy:** Refine existing algorithms with targeted improvements

**Steps:**
1. **Audio Preprocessing:**
   - Add noise reduction (noisereduce)
   - Adaptive normalization per segment
   - Better silence detection (webrtcvad)

2. **Feature Engineering:**
   - Test MFCCs, spectral rolloff, onset strength
   - Use scikit-learn to find best feature combinations
   - Cross-validate on diverse podcast genres

3. **Scoring Refinement:**
   - Collect user feedback (thumbs up/down on clips)
   - Use feedback to adjust scoring weights
   - A/B test different scoring formulas

4. **Boundary Optimization:**
   - VAD for precise clip edges
   - Smart padding (extend if mid-sentence)
   - Context awareness (don't cut off punchlines)

5. **Performance Optimization:**
   - Parallel processing (multiprocessing.Pool)
   - GPU acceleration for spectral analysis (CuPy)
   - Caching (save features for re-analysis)

**Pros:**
- Low risk, iterative improvements
- No major architecture changes
- Validates current approach
- Faster time to market

**Cons:**
- Incremental gains (10-20% accuracy improvement)
- May hit ceiling without ML

**Research Questions:**
- Which preprocessing step has biggest impact? (Noise reduction? Normalization?)
- Can we reduce processing time by 50% with optimizations?
- What are the most important features? (Energy? Pace? Spectral?)

### Path 5: **Platform Integration** (Low Risk, High Monetization)

**Strategy:** Focus on workflow, not detection accuracy

**Steps:**
1. **Social Media Integration:**
   - TikTok API: Auto-upload with captions
   - Instagram Graph API: Post to Reels
   - YouTube Data API: Upload Shorts

2. **CMS Integration:**
   - WordPress plugin
   - Webflow integration
   - Notion database sync

3. **Analytics Dashboard:**
   - Track which clips perform best
   - A/B test titles/thumbnails
   - ROI calculator (time saved = money earned)

4. **Collaboration Features:**
   - Team accounts (agencies)
   - Approval workflows
   - Comment/feedback on clips

5. **Branding Tools:**
   - Custom intro/outro templates
   - Watermark overlay
   - Auto-captions with styling

**Pros:**
- Doesn't require detection improvements
- High perceived value (saves manual work)
- Recurring revenue potential (SaaS)
- Sticky product (workflow lock-in)

**Cons:**
- Requires API integrations (complex, maintenance)
- OAuth flows, rate limits, API changes
- Doesn't improve core detection quality

**Research Questions:**
- Which platforms do users post to most? (Survey)
- What's the willingness to pay for integrations? (Pricing research)
- Can we partner with platforms for official integration?

---

## ğŸ”¬ TOP 10 RESEARCH QUESTIONS (Prioritized by Impact)

### ğŸ¥‡ Tier 1: Critical (Make or Break)

1. **Validation Dataset Creation**
   - **Question:** How do we define and measure "viral-worthy" accurately?
   - **Action:** Create dataset of 1000+ podcast clips with social media performance metrics
   - **Tools:** Web scraping (TikTok, YouTube Shorts), manual curation, user-contributed
   - **Expected Impact:** Objective accuracy measurement, training data for ML
   - **Timeline:** 2-4 weeks
   - **Cost:** $500-$2000 (manual labeling or paid dataset)

2. **Pattern Detection Accuracy**
   - **Question:** Which patterns (payoff, monologue, laughter) have highest viral correlation?
   - **Action:** A/B test with real users, track which clips they export/post
   - **Tools:** Analytics integration, user surveys
   - **Expected Impact:** Focus development on high-value patterns
   - **Timeline:** 4-8 weeks (need user base)
   - **Cost:** Free (analytics)

3. **Audio Feature Importance**
   - **Question:** Which audio features best predict engagement (RMS, ZCR, spectral, temporal)?
   - **Action:** Feature engineering + regression analysis on labeled dataset
   - **Tools:** scikit-learn, librosa, pandas
   - **Expected Impact:** 10-20% accuracy improvement
   - **Timeline:** 1-2 weeks
   - **Cost:** Free

### ğŸ¥ˆ Tier 2: High Value (Significant Improvements)

4. **Local vs Cloud Transcription**
   - **Question:** Should we use local Whisper (free, slower) or OpenAI API (paid, faster)?
   - **Action:** Benchmark faster-whisper, whisper.cpp vs OpenAI API
   - **Metrics:** Speed, accuracy (WER), cost, memory usage
   - **Expected Impact:** $0.36 â†’ $0 per video OR 2x faster processing
   - **Timeline:** 1 week
   - **Cost:** Free (test scripts)

5. **GPU Acceleration Feasibility**
   - **Question:** Can GPU acceleration (CuPy, CUDA) provide 3-5x speedup?
   - **Action:** Profile bottlenecks, implement GPU versions of key functions
   - **Tools:** nvprof, CuPy, PyTorch
   - **Expected Impact:** 3-5x faster processing (60s â†’ 15s for 1hr podcast)
   - **Timeline:** 2-3 weeks
   - **Cost:** Free (if have GPU) or $100-$200 (cloud GPU testing)

6. **Multiprocessing Parallelization**
   - **Question:** Can we parallelize pattern detection across CPU cores?
   - **Action:** Refactor to use multiprocessing.Pool, test on multi-core systems
   - **Tools:** Python multiprocessing, concurrent.futures
   - **Expected Impact:** 2-4x faster processing on multi-core systems
   - **Timeline:** 1-2 weeks
   - **Cost:** Free

### ğŸ¥‰ Tier 3: Nice to Have (Polish & Differentiation)

7. **Visual Feature Integration**
   - **Question:** Do facial expressions/gestures improve viral prediction?
   - **Action:** Prototype with DeepFace emotion recognition, correlate with engagement
   - **Tools:** OpenCV, DeepFace, MediaPipe
   - **Expected Impact:** Potential major differentiator, but uncertain ROI
   - **Timeline:** 3-4 weeks
   - **Cost:** Free (open-source models)

8. **Real-Time Streaming Analysis**
   - **Question:** Can we show clips incrementally as video processes?
   - **Action:** Implement sliding-window detection, streaming results
   - **Tools:** Python generators, websockets
   - **Expected Impact:** Perceived 50% faster (better UX)
   - **Timeline:** 2-3 weeks
   - **Cost:** Free

9. **Hook Optimization Research**
   - **Question:** What makes a strong hook for TikTok vs YouTube Shorts vs Instagram Reels?
   - **Action:** Analyze top 100 viral clips on each platform, extract patterns
   - **Tools:** Social media scraping, feature analysis
   - **Expected Impact:** Platform-specific scoring, better titles
   - **Timeline:** 2-4 weeks
   - **Cost:** $200-$500 (scraping tools/APIs)

10. **Speaker Diarization Value**
    - **Question:** Do users want to filter by speaker? (e.g., "Find Joe's best moments")
    - **Action:** User surveys, prototype with pyannote.audio
    - **Tools:** pyannote.audio, user interviews
    - **Expected Impact:** New feature for multi-host podcasts
    - **Timeline:** 2-3 weeks
    - **Cost:** Free (open-source)

---

## ğŸ“ˆ Recommended Development Roadmap

### Phase 1: Validation & Optimization (4-6 weeks)

**Goal:** Validate core algorithms, optimize performance

1. âœ… Create validation dataset (1000+ labeled clips)
2. âœ… Measure current accuracy baseline
3. âœ… Feature importance analysis (which audio features matter most)
4. âœ… Implement multiprocessing parallelization
5. âœ… GPU acceleration prototyping
6. âœ… A/B test pattern detectors (payoff vs monologue vs laughter)

**Deliverables:**
- Accuracy report (% viral prediction rate)
- Performance benchmarks (processing time vs accuracy)
- Optimized detection pipeline (2-4x faster)

### Phase 2: AI Enhancement (2-4 weeks)

**Goal:** Improve semantic understanding

1. âœ… Local Whisper integration (faster-whisper)
2. âœ… Sentiment analysis (HuggingFace transformers)
3. âœ… Speaker diarization (pyannote.audio)
4. âœ… Keyword extraction (spaCy, BERT)
5. âœ… Test AI scoring weights (algorithm vs AI balance)

**Deliverables:**
- Cost-effective transcription (local vs API decision)
- Semantic filtering (search by topic/keyword)
- Speaker-specific clips (multi-host support)

### Phase 3: User Experience (3-4 weeks)

**Goal:** Polish UI, improve workflow

1. âœ… In-app video preview (Web Audio API)
2. âœ… Real-time waveform visualization (Canvas/WebGL)
3. âœ… Batch processing UI (queue system)
4. âœ… Clip templates (intros, captions, watermarks)
5. âœ… Export presets (TikTok, YouTube Shorts, Instagram Reels)

**Deliverables:**
- Polished UI with video preview
- Batch mode for agencies
- Social media export presets

### Phase 4: Platform Integration (4-6 weeks)

**Goal:** Workflow automation, monetization

1. âœ… TikTok API integration (auto-upload)
2. âœ… Instagram Graph API (Reels posting)
3. âœ… YouTube Data API (Shorts upload)
4. âœ… Analytics dashboard (clip performance tracking)
5. âœ… SaaS infrastructure (user accounts, subscriptions)

**Deliverables:**
- One-click publishing to social media
- Performance analytics
- Monetization-ready SaaS product

---

## ğŸš€ Quick Win Experiments (Do These First)

### Experiment 1: Feature Ablation Study (1 week)

**Goal:** Which audio features actually matter?

**Method:**
1. Take 100 clips (50 viral, 50 non-viral)
2. Calculate all features: RMS, ZCR, spectral_centroid, spectral_contrast, MFCCs, tempo
3. Train simple logistic regression: viral = f(features)
4. Check feature importance (coefficients)
5. Remove low-importance features from pipeline

**Expected Outcome:** Simpler, faster detection with same accuracy

### Experiment 2: Hook Strength Validation (3 days)

**Goal:** Does hook strength actually predict social media performance?

**Method:**
1. Take 50 viral TikTok clips, calculate hook score
2. Compare to control group of 50 random clips
3. Measure correlation: hookScore vs (views, likes, shares)
4. Adjust scoring weights based on findings

**Expected Outcome:** Optimized hook scorer, or discover it's not important

### Experiment 3: Local Whisper Benchmark (2 days)

**Goal:** Is local Whisper fast enough?

**Method:**
1. Test faster-whisper on 10 podcast episodes (various lengths)
2. Measure: accuracy (WER vs OpenAI), speed (seconds), memory (GB)
3. Calculate cost savings: $0.36/video Ã— expected usage
4. Decision: local or API?

**Expected Outcome:** Clear decision on transcription strategy

### Experiment 4: Multiprocessing Speedup (3 days)

**Goal:** Measure actual speedup from parallelization

**Method:**
1. Refactor detect_payoff_moments() to process chunks in parallel
2. Test on 2-core, 4-core, 8-core, 16-core systems
3. Measure speedup vs overhead
4. Implement if > 2x speedup on 4+ cores

**Expected Outcome:** 2-4x faster processing on multi-core systems

### Experiment 5: User Preference Study (1 week)

**Goal:** What do users actually want?

**Method:**
1. Recruit 10 podcast creators
2. Show them detected clips (no titles)
3. Ask: "Would you post this?" (yes/no)
4. Measure: current_accuracy = % yes votes
5. Ask: "What would make this better?" (open-ended)

**Expected Outcome:** Validated accuracy, feature prioritization

---

## ğŸ CONCLUSION: The Optimal Path Forward

### Recommended Strategy: **Hybrid Optimization + Validation-Driven Development**

**Why this path:**
1. **Low Risk:** Builds on working foundation
2. **High ROI:** 10-20% accuracy gains with minimal complexity
3. **Fast Iteration:** Can ship improvements weekly
4. **User-Driven:** Validation dataset guides decisions
5. **Monetizable:** Can launch SaaS while improving detection

### 90-Day Plan

**Weeks 1-2: Validation**
- Create labeled dataset (200 clips)
- Measure current accuracy baseline
- Identify biggest failure modes

**Weeks 3-4: Optimization**
- Feature importance analysis
- Multiprocessing parallelization
- GPU acceleration (if beneficial)

**Weeks 5-6: AI Enhancement**
- Local Whisper integration
- Sentiment analysis prototype
- Speaker diarization test

**Weeks 7-8: UI Polish**
- In-app video preview
- Real-time waveform
- Batch processing UI

**Weeks 9-10: Integration**
- TikTok API integration
- Export presets
- Analytics dashboard

**Weeks 11-12: Launch**
- Beta testing (50 users)
- Performance monitoring
- Iterate based on feedback

### Success Criteria (End of 90 Days)

âœ… **Accuracy:** 80%+ viral prediction rate (validated on test set)  
âœ… **Speed:** 30-60s for 1hr podcast (2-4x improvement)  
âœ… **UX:** 4.5+ star rating from beta users  
âœ… **Monetization:** 100+ waitlist signups for paid version  

---

## ğŸ“š Appendix: Technology Deep Dives

### A. Librosa Audio Features Explained

| Feature | What It Measures | Use Case in Our System |
|---------|------------------|------------------------|
| **RMS** | Energy/loudness | Detect silence, energy spikes, volume levels |
| **Zero-Crossing Rate** | Frequency of sign changes | Speech pace proxy (fast speech = high ZCR) |
| **Spectral Centroid** | "Brightness" (frequency center) | Laughter detection (bright = high centroid) |
| **Spectral Contrast** | Difference between peaks/valleys | Hook strength (high contrast = engaging) |
| **MFCCs** | Timbral texture | Voice vs music, speaker identification |
| **Tempo** | Beats per minute | Music detection, energy level |
| **Onset Strength** | Rate of energy increases | Rhythmic patterns, emphasis detection |

### B. FFmpeg Command Reference

```bash
# Extract audio only
ffmpeg -i input.mp4 -vn -acodec pcm_s16le -ar 22050 -ac 1 audio.wav

# Fast clip export (stream copy, no re-encode)
ffmpeg -ss 00:02:30 -i input.mp4 -t 00:00:15 -c copy clip.mp4

# Accurate clip export (re-encode)
ffmpeg -i input.mp4 -ss 00:02:30 -t 00:00:15 \
  -c:v libx264 -preset medium -crf 23 -c:a aac -b:a 192k clip.mp4

# Hardware encoding (NVIDIA GPU)
ffmpeg -i input.mp4 -ss 00:02:30 -t 00:00:15 \
  -c:v h264_nvenc -preset p4 -cq 23 -c:a aac clip.mp4

# Concatenate clips (remove dead space)
ffmpeg -i input.mp4 -filter_complex "
  [0:v]trim=0:30[v0];[0:v]trim=40:80[v1];[v0][v1]concat=n=2:v=1:a=0[vout];
  [0:a]atrim=0:30[a0];[0:a]atrim=40:80[a1];[a0][a1]concat=n=2:v=0:a=1[aout]
" -map "[vout]" -map "[aout]" output.mp4

# Add captions from Whisper JSON
ffmpeg -i input.mp4 -vf "subtitles=captions.srt" output.mp4

# Add watermark overlay
ffmpeg -i input.mp4 -i logo.png -filter_complex "overlay=W-w-10:10" output.mp4
```

### C. OpenAI API Cost Breakdown (2026 Pricing)

| Service | Model | Pricing | Example Cost (1hr podcast) |
|---------|-------|---------|---------------------------|
| **Whisper** | whisper-1 | $0.006/min | 60 min Ã— $0.006 = **$0.36** |
| **GPT Text** | gpt-4o-mini | $0.150/1M input tokens<br>$0.600/1M output tokens | 10 clips Ã— ~2K tokens = **$0.02** |
| **Embeddings** | text-embedding-3-small | $0.020/1M tokens | Optional feature = **$0.01** |

**Total per video:** ~$0.40-$0.50 (with AI enhancement)

### D. Competitor Analysis

| Tool | Approach | Pros | Cons | Price |
|------|----------|------|------|-------|
| **OpusClip** | AI-powered (proprietary) | High accuracy, auto-captions | Expensive, cloud-only | $29/mo |
| **Descript** | Transcript-first editing | Full editor, collaboration | Overkill for clips, steep learning curve | $24/mo |
| **Kapwing** | Manual + AI assist | Easy to use, web-based | Low automation, manual review needed | $16/mo |
| **Zubtitle** | Auto-captions + templates | Fast, simple | No clip detection, manual selection | $19/mo |
| **Our Tool** | Hybrid (algorithms + AI) | Fast, accurate, affordable | Early stage, needs validation | TBD |

**Market Gap:** High-quality automated clip detection without expensive AI compute costs.

---

## ğŸ“ Key Learnings & Insights

1. **Audio analysis is 80% of the solution** â€“ Visual features are nice-to-have, not must-have
2. **Local baseline > global percentiles** â€“ PodFlow's adaptive thresholds are superior
3. **Hook strength matters** â€“ First 3 seconds determine social media success
4. **AI is validation, not detection** â€“ Algorithms are fast/free, AI adds context
5. **Processing time < 2 minutes is critical** â€“ Users won't wait longer
6. **Transcription is the most expensive operation** â€“ Optimize this first
7. **FFmpeg is the bottleneck for exports** â€“ GPU encoding can 10x this
8. **Zustand is sufficient for state** â€“ No need for complex state management
9. **IPC is a bottleneck** â€“ Throttle progress updates, batch results
10. **Users need confidence scores** â€“ "87% viral potential" is more actionable than raw scores

---

## ğŸ“ Next Steps

1. **Create validation dataset** â†’ Measure current accuracy
2. **Run quick win experiments** â†’ Feature ablation, hook validation
3. **Implement multiprocessing** â†’ 2-4x speedup
4. **Local Whisper integration** â†’ Cut costs to $0
5. **Beta launch** â†’ 50 users, collect feedback
6. **Iterate based on data** â†’ Don't guess, measure

**Most Important:** Start with validation. Everything else depends on knowing what "good" means.

---

*Last Updated: 2026-01-18*  
*Version: 1.0*  
*Authors: Architecture documentation generated for Carson Construct 2 project*
